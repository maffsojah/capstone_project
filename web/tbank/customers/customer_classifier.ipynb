{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from time import time\n",
    "\n",
    "spark = SparkSession.builder.setMaster('local[*]').appName('Multiclass Classification: TBank').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load train data : 1 million rows\n",
    "TrainSet = spark.read.csv('hdfs://localhost:9000/user/hduser/datasets/oneMill.csv', header='true', inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+-------+--------------------+----------+------+------------------+----------------+-------+------------------+-------------+\n",
      "|Gender|   Account_Type|    Age|           Education|Employment|Salary|Employer_Stability|Customer_Loyalty|Balance|Residential_Status|Service_Level|\n",
      "+------+---------------+-------+--------------------+----------+------+------------------+----------------+-------+------------------+-------------+\n",
      "|  Male|Current Account|   60 +|Highschool and below|  Contract| 29633|            Stable|               8|    386|             Owned|          1.0|\n",
      "|  Male|Current Account|   60 +|Highschool and below|   Student|  5622|            Stable|               0|    386|             Owned|          0.0|\n",
      "|  Male|Savings Account|   60 +|  Tertiary and above|  Contract|  5622|          Unstable|               5| 269684|             Owned|          0.0|\n",
      "|Female|Current Account|18 - 35|Highschool and below| Permanent|  5622|            Stable|               5| 269684|             Owned|          1.0|\n",
      "|  Male|Savings Account|   60 +|Highschool and below|  Contract|  5622|          Unstable|               9|   8024|             Owned|          1.0|\n",
      "|  Male|Current Account|36 - 59|  Tertiary and above|  Contract|  5622|          Unstable|               4|    386|             Owned|          0.0|\n",
      "|Female|Savings Account|   60 +|  Tertiary and above| Permanent|   236|            Stable|               5| 269684|             Owned|          1.0|\n",
      "|  Male|Current Account|18 - 35|Highschool and below|  Contract|  5622|            Stable|               4| 269684|             Owned|          1.0|\n",
      "|Female|Current Account|   60 +|Highschool and below|  Contract|  5622|            Stable|               6|    386|            Rented|          0.0|\n",
      "|  Male|Current Account|18 - 35|  Tertiary and above|   Student|  5622|            Stable|               0| 269684|            Rented|          0.0|\n",
      "|  Male|Savings Account|36 - 59|Highschool and below|  Contract|   236|            Stable|               0| 269684|             Owned|          0.0|\n",
      "|  Male|Current Account|18 - 35|Highschool and below| Permanent|   236|          Unstable|               9|    386|             Owned|          1.0|\n",
      "|  Male|Savings Account|18 - 35|Highschool and below|   Student|  5622|          Unstable|               0|   8024|            Rented|          0.0|\n",
      "|Female|Savings Account|18 - 35|  Tertiary and above|  Contract| 29633|            Stable|               4|    386|             Owned|          1.0|\n",
      "|Female|Current Account|18 - 35|  Tertiary and above| Permanent| 29633|          Unstable|               4| 269684|            Rented|          1.0|\n",
      "|Female|Current Account|36 - 59|  Tertiary and above|  Contract| 29633|            Stable|               5|   8024|            Rented|          1.0|\n",
      "|Female|Current Account|36 - 59|  Tertiary and above| Permanent| 29633|            Stable|               9| 269684|            Rented|          2.0|\n",
      "|Female|Current Account|36 - 59|  Tertiary and above| Permanent|  5622|          Unstable|               8|    386|             Owned|          1.0|\n",
      "|  Male|Current Account|18 - 35|  Tertiary and above|  Contract| 29633|          Unstable|               3|   8024|             Owned|          0.0|\n",
      "|  Male|Current Account|36 - 59|Highschool and below| Permanent| 29633|          Unstable|               6|    386|            Rented|          0.0|\n",
      "+------+---------------+-------+--------------------+----------+------+------------------+----------------+-------+------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating Spark SQL temporary views with the DataFrames\n",
    "## Train View\n",
    "TrainSet.createOrReplaceTempView(\"customers\")\n",
    "\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "## Train\n",
    "results = spark.sql(\"SELECT Gender, Account_Type, Age, Education, Employment, Salary, Employer_Stability, Customer_Loyalty, Balance, Residential_Status, Service_Level FROM customers\")\n",
    "\n",
    "\n",
    "\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Gender: string, Account_Type: string, Age: string, Education: string, Employment: string, Salary: int, Employer_Stability: string, Customer_Loyalty: int, Balance: int, Residential_Status: string, Service_Level: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#results columns: trainset\n",
    "cols = results.columns\n",
    "\n",
    "\n",
    "## tests columns\n",
    "#testcols = tests.columns\n",
    "\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, features: vector, Gender: string, Account_Type: string, Age: string, Education: string, Employment: string, Salary: int, Employer_Stability: string, Customer_Loyalty: int, Balance: int, Residential_Status: string, Service_Level: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "categoricalColumns = [\"Gender\", \"Account_Type\", \"Age\",\"Education\", \"Employment\", \"Employer_Stability\", \"Residential_Status\"]\n",
    "stages = [] # stages in the pipeline\n",
    "\n",
    "# \"Gender\",\"Account_Type\",\n",
    "\n",
    "for categoricalCol in categoricalColumns:\n",
    "    \n",
    "    # Category Indexing with StringIndexer\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n",
    "    \n",
    "    # Using OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "    #encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\")\n",
    "    encoder = OneHotEncoder(inputCol=stringIndexer.getOutputCol(), outputCol=categoricalCol+\"classVec\")\n",
    "    \n",
    "    # Adding the stages: will be run all at once later on\n",
    "    stages += [stringIndexer, encoder]\n",
    "    \n",
    "# convert label into label indices using the StringIndexer\n",
    "label_stringIdx = StringIndexer(inputCol = \"Service_Level\", outputCol = \"label\")\n",
    "stages += [label_stringIdx]\n",
    "\n",
    "# Transform all features into a vector using VectorAssembler\n",
    "numericCols = [\"Salary\", \"Customer_Loyalty\", \"Balance\"]\n",
    "assemblerInputs = map(lambda c: c + \"classVec\", categoricalColumns) + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "\n",
    "# Creating a Pipeline for Training\n",
    "pipeline = Pipeline(stages=stages)\n",
    "# Running the feature transformations.\n",
    "# - fit() computes feature statistics as needed\n",
    "# - transform() actually transforms the features\n",
    "pipelineModel = pipeline.fit(results)\n",
    "results = pipelineModel.transform(results)\n",
    "\n",
    "# Keep relevant columns\n",
    "selectedcols = [\"label\", \"features\"] + cols\n",
    "TrainingData = results.select(selectedcols)\n",
    "display(TrainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "699367\n",
      "300633\n"
     ]
    }
   ],
   "source": [
    "# Splitting data randomly into training and test sets. set seed for reproducibility\n",
    "(trainData, testData) = TrainingData.randomSplit([0.7, 0.3], seed = 100)\n",
    "\n",
    "print trainData.count()\n",
    "print testData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Test Error = 0.0768046 \n",
      "Accuracy = 92.3195 \n",
      "Coefficients: \n",
      "DenseMatrix([[  2.12804972e-03,   4.06990620e-03,   9.24882853e-01,\n",
      "               -9.19501071e-01,  -8.85696473e-01,   1.66742461e+00,\n",
      "                7.41877324e-01,   8.86480653e-01,   8.83721151e-01,\n",
      "               -5.23700051e-05,  -8.69021241e-01,  -5.04678529e-06],\n",
      "             [  3.24168203e-04,  -1.97462169e-03,  -4.87435214e-02,\n",
      "                4.70586078e-02,   5.65099699e-02,  -1.05953923e-01,\n",
      "               -4.05245340e-02,  -5.19981488e-02,  -4.51929447e-02,\n",
      "                3.45664886e-06,   1.63442644e-01,   3.06447794e-07],\n",
      "             [ -2.45221792e-03,  -2.09528451e-03,  -8.76139331e-01,\n",
      "                8.72442463e-01,   8.29186503e-01,  -1.56147068e+00,\n",
      "               -7.01352790e-01,  -8.34482504e-01,  -8.38528207e-01,\n",
      "                4.89133563e-05,   7.05578597e-01,   4.74033749e-06]])\n",
      "Intercept: [6.1363818633,0.0160259475899,-6.15240781089]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'temp_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-2ef55fd26f5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/lr_model'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mregModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mmodel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegressionModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'temp_path' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "reg = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=1000, regParam=0.01, family=\"multinomial\" )\n",
    "regModel = reg.fit(trainData)\n",
    "\n",
    "predict = regModel.transform(testData)\n",
    "predict.select(\"prediction\", \"label\", \"features\").show()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predict)\n",
    "\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "print(\"Accuracy = %g \" % (accuracy * 100))\n",
    "print(\"Coefficients: \\n\" + str(regModel.coefficientMatrix))\n",
    "print(\"Intercept: \" + str(regModel.interceptVector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tempfile\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "\n",
    "temp_path = tempfile.mkdtemp()\n",
    "#globs['temp_path'] = temp_path\n",
    "reg_path = temp_path + '/reg'\n",
    "reg.save(reg_path)\n",
    "model2 = LogisticRegression.load(reg_path)\n",
    "model2.getMaxIter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'overwrite'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-999f149157de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'reg_model'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mregModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegressionModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mregModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoefficientMatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoefficientMatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'overwrite'"
     ]
    }
   ],
   "source": [
    "model_path = temp_path + 'reg_model'\n",
    "regModel.save(model_path)\n",
    "model2 = LogisticRegressionModel.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regModel.coefficientMatrix[0, 1] == model2.coefficientMatrix[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regModel.interceptVector == model2.interceptVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
