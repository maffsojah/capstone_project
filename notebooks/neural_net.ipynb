{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maffsojah/anaconda2/lib/python2.7/site-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['random', 'add']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('Bank-Customer-Classifier') \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "sc = pyspark.SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import random\n",
    "import math\n",
    "from operator import add\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_training_set():\n",
    "    X = np.load(\"../datasets/X.npy\")\n",
    "    y = np.load(\"../datasets/y.npy\")\n",
    "    dataset = []\n",
    "    for i in range(y.shape[0]):\n",
    "        x_vector = X[i].reshape((-1, 1))\n",
    "        y_vector = np.eye(10)[y[i]-1].reshape((-1, 1))\n",
    "        dataset.append((x_vector, y_vector))\n",
    "        \n",
    "    return dataset    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parallelize data into spark nodes.\n",
    "\n",
    "def parse_data():\n",
    "    dataset = load_training_set()\n",
    "    data_set = sc.parallelize(dataset)\n",
    "    #split it into training, validation and test sets. Use the randomSplit method\n",
    "    weights = [.8, .1, .1]\n",
    "    seed = 8888\n",
    "    train_set, validate_set, test_set = data_set.randomSplit(weights, seed)\n",
    "    \n",
    "    return (train_set, validate_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# derivative of the sigmoid functionin terms of the output\n",
    "def dsigmoid(y):\n",
    "    gz = sigmoid(y)\n",
    "    return gz * (1.0 - gz)\n",
    "\n",
    "def sigmoid(x):\n",
    "    # exp function provided by numpy can support vector operation by default\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the class is the NN\n",
    "class NN:\n",
    "    def __init__(self, ni, nh, no):\n",
    "        # inputs, hidden, and output nodes\n",
    "        self.n1 = ni + 1 # +1 for bias node\n",
    "        self.n2 = nh\n",
    "        self.n3 = no\n",
    "        \n",
    "        # weights variables (theta) in the model\n",
    "        self.w1 = self.weights_init(ni, nh)\n",
    "        self.w2 = self.weights_initi(nh, no)\n",
    "        \n",
    "        # accumulating the gradient from all the train samples\n",
    "        self.Delta1 = np.zeros(shape=(self.n2, self.n1)) # for w1\n",
    "        self.Delta2 = np.zeros(shape=(self.n3, self.n2+1)) # for w2\n",
    "        \n",
    "        def weights_init(self, l_in, l_out):\n",
    "            eps_init = 0.12\n",
    "            ret = mp.random.rand(l_out, 1+l_in) * 2 * eps_init - eps_init\n",
    "            return ret\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for forward and back propagation and returning all Deltas \n",
    "# as well as prediction error for display.\n",
    "def ann_train_eval(w, sample):\n",
    "    w1, w2 = w\n",
    "    x = sample[0]\n",
    "    y = sample[1]\n",
    "    a1 = np.vstack(([1.0], x))\n",
    "    \n",
    "    # hidden activations\n",
    "    z2 = np.dot(w1, a1)\n",
    "    a2 = np.vstack(([1.0], sigmoid(z2)))\n",
    "    \n",
    "    # output activations\n",
    "    z3 = np.dot(w2, a2)\n",
    "    a3 = sigmoid(z3)\n",
    "    \n",
    "    # starting backpropagation and calculating error terms for output\n",
    "    delta3 = a3 - y\n",
    "    \n",
    "    # skipping first column which is for bias\n",
    "    delta2 = (np.dot(w2.T, delta3))[1:] * dsigmoid(z2)\n",
    "    \n",
    "    # gradient from all train samples for accumulating\n",
    "    Delta1 = np.dot(delta2, a1.T)\n",
    "    Delta2 = np.dot(delta3, a2.T)\n",
    "    \n",
    "    # mse for calculating train predict error, mse was used, just to show the minimization, \n",
    "    # if using other optimization method, should use cost function\n",
    "    \n",
    "    return np.array([Delta1, Delta2, np.mean((a3 - y)**2)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function is provided to do prediction by using a trained neural network model\n",
    "def nn_predict(ann, inputs):\n",
    "    x = inputs.reshape((-1, 1))\n",
    "    a1 = np.vstack(([1.0], x))\n",
    "\n",
    "    # hidden activations\n",
    "    z2 = np.dot(ann.w1, a1)\n",
    "    a2 = np.vstack(([1.0], sigmoid(z2)))\n",
    "\n",
    "    # output activations\n",
    "    z3 = np.dot(ann.w2, a2)\n",
    "    a3 = sigmoid(z3)\n",
    "    return a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_train(ann, train_set, max_iter):\n",
    "    Lambda = 10.01 # Regularization parameter (to avoid overfit issue)\n",
    "    m = train_set.count()\n",
    "    for iteration in range(max_iter):\n",
    "        eval_res = train_set.map(lambda x: ann_train_eval((ann.w1, ann.w2), x))\n",
    "        \n",
    "        # calculate derivation of weights via average bp results\n",
    "        average_eval = eval_res.reduce(add) / train_set.count()\n",
    "        dw1 = average_eval[0]\n",
    "        dw2 = average_eval[1]\n",
    "        \n",
    "        # mean error sololy for display\n",
    "        mean_err = average_eval[2]\n",
    "        \n",
    "        # for all the weights, you should not apply regulization on the first column since they are for bias\n",
    "        for i in range(ann.w1.shape[0]):\n",
    "            ann.w1[i, 0] = ann.w1[i, 0] - dw1[i, 0]\n",
    "        for i in range(ann.w1.shape[0]):\n",
    "            for j in range(1, ann.w1.shape[1]):\n",
    "                # here learn rate is 1.0, Lambda is the Regularization parameter\n",
    "                ann.w1[i, j] = ann.w1[i, j] - (dw1[i, j] + (Lambda/m) * ann.w1[i, j])\n",
    "        \n",
    "        for i in range(ann.w2.shape[0]):\n",
    "            ann.w2[i, 0] = ann.w2[i, 0] - dw2[i, 0]\n",
    "        for i in range(ann.w2.shape[0]):\n",
    "            for j in range(1, ann.w2.shape[1]):\n",
    "                ann.w2[i, j] = ann.w2[i, j] - (dw2[i, j] + (Lambda/m) * ann.w2[i, j])\n",
    "        if 0 == iteration % 50:\n",
    "            print (\"mean error p\", mean_err)\n",
    "    return ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unbound method parallelize() must be called with SparkContext instance as first argument (got list instance instead)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-009ab5791114>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load and parse data, randomly devide dataset to three parts for train, validate and test separately\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#print train_set.count(), validate_set.count(), test_set.count()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-51c2568bbf35>\u001b[0m in \u001b[0;36mparse_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparse_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_training_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdata_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m#split it into training, validation and test sets. Use the randomSplit method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unbound method parallelize() must be called with SparkContext instance as first argument (got list instance instead)"
     ]
    }
   ],
   "source": [
    "# load and parse data, randomly devide dataset to three parts for train, validate and test separately \n",
    "train_set, validate_set, test_set = parse_data()\n",
    "#print train_set.count(), validate_set.count(), test_set.count()\n",
    "\n",
    "n = NN(400, 50, 10)\n",
    "\n",
    "# train it with some patterns\n",
    "dt_st = datetime.datetime.now()\n",
    "print (\"train start at: {0}\".format(dt_st))\n",
    "\n",
    "nt = nn_train(n, train_set, 400)\n",
    "\n",
    "dt_end = datetime.datetime.now()\n",
    "print (\"train end at: {0}\".format(dt_end))\n",
    "print (\"time elapse in traininf: {0} seconds\".format((dt_end-dt_st).seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-daec8cf9130d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# test trained neural network model on train dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mn_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mval_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mactual_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_set' is not defined"
     ]
    }
   ],
   "source": [
    "# test trained neural network model on train dataset\n",
    "n_set = train_set.count()\n",
    "val_res = train_set.map(lambda x : 1 + np.argmax(nn_predict(nt, x[0]))).collect()\n",
    "actual_res = train_set.map(lambda x : 1 + np.argmax(x[1])).collect()\n",
    "\n",
    "accurate = 0\n",
    "for idx in range(n_set):\n",
    "    if val_res[idx] == actual_res[idx]:\n",
    "        accurate += 1\n",
    "print(\"train set accuracy: {0} %\".format(100.0 * accurate / n_set))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test trained neural network model on validation data set\n",
    "n_val_set = validate_set.count()\n",
    "val_res = validate_set.map(lambda x : 1 + np.argmax(nn_predict(nt, x[0]))).collect()\n",
    "actual_res = validate_set.map(lambda x : 1 + np.argmax(x[1])).collect()\n",
    "\n",
    "accurate = 0\n",
    "for idx in range(n_val_set):\n",
    "    if val_res[idx] == actual_res[idx]:\n",
    "        accurate += 1\n",
    "print(\"validation set accuracy: {0} %\".format(100.0 * accurate / n_val_set))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test trained neural network model on validation data set\n",
    "n_val_set = test_set.count()\n",
    "\n",
    "val_res = test_set.map(lambda x : 1 + np.argmax(nn_predict(nt, x[0]))).collect()\n",
    "actual_res = test_set.map(lambda x : 1 + np.argmax(x[1])).collect()\n",
    "\n",
    "accurate = 0\n",
    "for idx in range(n_val_set):\n",
    "    if val_res[idx] == actual_res[idx]:\n",
    "        accurate += 1\n",
    "print(\"test set accuracy: {0} %\".format(100.0 * accurate / n_val_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e6e5ec615459>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Loading the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../datasets/dummy.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Loading the data\n",
    "\n",
    "data = spark.read.format('csv').load('../datasets/dummy.csv')\n",
    "\n",
    "\n",
    "# splitting the data\n",
    "splits = data.randomSplit([1])\n",
    "train = splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 8 input layers(features), 9 and 8 hidden layers and 3 output classes\n",
    "\n",
    "layers = [8, 9, 8, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MultilayerPerceptronClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-264fe5c6037f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# trainer with parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultilayerPerceptronClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxIter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblockSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MultilayerPerceptronClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "# trainer with parameters\n",
    "\n",
    "trainer = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128)\n",
    "\n",
    "\n",
    "# training the model\n",
    "model = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
