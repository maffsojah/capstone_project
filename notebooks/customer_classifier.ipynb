{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from time import time\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('Multiclass Classification: TBank').config(\"spark.sql.warehouse.dir\", \"/home/maffsojah/Projects/HIT_400/capstone_project/web/tbank/spark-warehouse\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load train data : 1 million rows\n",
    "TrainSet = spark.read.csv('hdfs://localhost:9000/user/hduser/datasets/oneMill.csv', header='true', inferSchema='true')\n",
    "#TrainSet = spark.read.csv('../datasets/predict.csv', header='true', inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+-------+--------------------+----------+------+------------------+----------------+-------+------------------+-------------+\n",
      "|Gender|   Account_Type|    Age|           Education|Employment|Salary|Employer_Stability|Customer_Loyalty|Balance|Residential_Status|Service_Level|\n",
      "+------+---------------+-------+--------------------+----------+------+------------------+----------------+-------+------------------+-------------+\n",
      "|  Male|Current Account|   60 +|Highschool and below|  Contract| 29633|            Stable|               8|    386|             Owned|          1.0|\n",
      "|  Male|Current Account|   60 +|Highschool and below|   Student|  5622|            Stable|               0|    386|             Owned|          0.0|\n",
      "|  Male|Savings Account|   60 +|  Tertiary and above|  Contract|  5622|          Unstable|               5| 269684|             Owned|          0.0|\n",
      "|Female|Current Account|18 - 35|Highschool and below| Permanent|  5622|            Stable|               5| 269684|             Owned|          1.0|\n",
      "|  Male|Savings Account|   60 +|Highschool and below|  Contract|  5622|          Unstable|               9|   8024|             Owned|          1.0|\n",
      "|  Male|Current Account|36 - 59|  Tertiary and above|  Contract|  5622|          Unstable|               4|    386|             Owned|          0.0|\n",
      "|Female|Savings Account|   60 +|  Tertiary and above| Permanent|   236|            Stable|               5| 269684|             Owned|          1.0|\n",
      "|  Male|Current Account|18 - 35|Highschool and below|  Contract|  5622|            Stable|               4| 269684|             Owned|          1.0|\n",
      "|Female|Current Account|   60 +|Highschool and below|  Contract|  5622|            Stable|               6|    386|            Rented|          0.0|\n",
      "|  Male|Current Account|18 - 35|  Tertiary and above|   Student|  5622|            Stable|               0| 269684|            Rented|          0.0|\n",
      "|  Male|Savings Account|36 - 59|Highschool and below|  Contract|   236|            Stable|               0| 269684|             Owned|          0.0|\n",
      "|  Male|Current Account|18 - 35|Highschool and below| Permanent|   236|          Unstable|               9|    386|             Owned|          1.0|\n",
      "|  Male|Savings Account|18 - 35|Highschool and below|   Student|  5622|          Unstable|               0|   8024|            Rented|          0.0|\n",
      "|Female|Savings Account|18 - 35|  Tertiary and above|  Contract| 29633|            Stable|               4|    386|             Owned|          1.0|\n",
      "|Female|Current Account|18 - 35|  Tertiary and above| Permanent| 29633|          Unstable|               4| 269684|            Rented|          1.0|\n",
      "|Female|Current Account|36 - 59|  Tertiary and above|  Contract| 29633|            Stable|               5|   8024|            Rented|          1.0|\n",
      "|Female|Current Account|36 - 59|  Tertiary and above| Permanent| 29633|            Stable|               9| 269684|            Rented|          2.0|\n",
      "|Female|Current Account|36 - 59|  Tertiary and above| Permanent|  5622|          Unstable|               8|    386|             Owned|          1.0|\n",
      "|  Male|Current Account|18 - 35|  Tertiary and above|  Contract| 29633|          Unstable|               3|   8024|             Owned|          0.0|\n",
      "|  Male|Current Account|36 - 59|Highschool and below| Permanent| 29633|          Unstable|               6|    386|            Rented|          0.0|\n",
      "+------+---------------+-------+--------------------+----------+------+------------------+----------------+-------+------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating Spark SQL temporary views with the DataFrames\n",
    "## Train View\n",
    "TrainSet.createOrReplaceTempView(\"customers\")\n",
    "# TrainSet.createOrReplaceTempView(\"predict\")\n",
    "\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "## Train\n",
    "results = spark.sql(\"SELECT Gender, Account_Type, Age, Education, Employment, Salary, Employer_Stability, Customer_Loyalty, Balance, Residential_Status, Service_Level FROM customers\")\n",
    "\n",
    "# results = spark.sql(\"SELECT Gender, Account_Type, Age, Education, Employment, Salary, Employer_Stability, Customer_Loyalty, Balance, Residential_Status, Service_Level FROM predict\")\n",
    "\n",
    "\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Gender: string, Account_Type: string, Age: string, Education: string, Employment: string, Salary: int, Employer_Stability: string, Customer_Loyalty: int, Balance: int, Residential_Status: string, Service_Level: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#results columns: trainset\n",
    "cols = results.columns\n",
    "\n",
    "\n",
    "## tests columns\n",
    "#testcols = tests.columns\n",
    "\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, features: vector, Gender: string, Account_Type: string, Age: string, Education: string, Employment: string, Salary: int, Employer_Stability: string, Customer_Loyalty: int, Balance: int, Residential_Status: string, Service_Level: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "categoricalColumns = [\"Gender\", \"Account_Type\", \"Age\",\"Education\", \"Employment\", \"Employer_Stability\", \"Residential_Status\"]\n",
    "stages = [] # stages in the pipeline\n",
    "\n",
    "# \"Gender\",\"Account_Type\",\n",
    "\n",
    "for categoricalCol in categoricalColumns:\n",
    "    \n",
    "    # Category Indexing with StringIndexer\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n",
    "    \n",
    "    # Using OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "    #encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\")\n",
    "    encoder = OneHotEncoder(inputCol=stringIndexer.getOutputCol(), outputCol=categoricalCol+\"classVec\")\n",
    "    \n",
    "    # Adding the stages: will be run all at once later on\n",
    "    stages += [stringIndexer, encoder]\n",
    "    \n",
    "# convert label into label indices using the StringIndexer\n",
    "label_stringIdx = StringIndexer(inputCol = \"Service_Level\", outputCol = \"label\")\n",
    "stages += [label_stringIdx]\n",
    "\n",
    "# Transform all features into a vector using VectorAssembler\n",
    "numericCols = [\"Salary\", \"Customer_Loyalty\", \"Balance\"]\n",
    "assemblerInputs = map(lambda c: c + \"classVec\", categoricalColumns) + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "\n",
    "# Creating a Pipeline for Training\n",
    "pipeline = Pipeline(stages=stages)\n",
    "# Running the feature transformations.\n",
    "# - fit() computes feature statistics as needed\n",
    "# - transform() actually transforms the features\n",
    "pipelineModel = pipeline.fit(results)\n",
    "results = pipelineModel.transform(results)\n",
    "\n",
    "# Keep relevant columns\n",
    "selectedcols = [\"label\", \"features\"] + cols\n",
    "TrainingData = results.select(selectedcols)\n",
    "display(TrainingData)\n",
    "# predictData = results.select(selectedcols)\n",
    "# display(predictData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "699367\n",
      "300633\n"
     ]
    }
   ],
   "source": [
    "# Splitting data randomly into training and test sets. set seed for reproducibility\n",
    "(trainData, testData) = TrainingData.randomSplit([0.7, 0.3], seed = 100)\n",
    "\n",
    "print trainData.count()\n",
    "print testData.count()\n",
    "# print(predictData).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "|       0.0|  0.0|(12,[0,1,2,4,9,11...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Test Error = 0.0768046 \n",
      "Accuracy = 92.3195 Percent\n",
      "Coefficients: \n",
      "DenseMatrix([[  2.12804969e-03,   4.06990616e-03,   9.24882853e-01,\n",
      "               -9.19501070e-01,  -8.85696473e-01,   1.66742461e+00,\n",
      "                7.41877324e-01,   8.86480653e-01,   8.83721151e-01,\n",
      "               -5.23700051e-05,  -8.69021241e-01,  -5.04678529e-06],\n",
      "             [  3.24168190e-04,  -1.97462168e-03,  -4.87435214e-02,\n",
      "                4.70586079e-02,   5.65099699e-02,  -1.05953923e-01,\n",
      "               -4.05245341e-02,  -5.19981488e-02,  -4.51929448e-02,\n",
      "                3.45664886e-06,   1.63442644e-01,   3.06447794e-07],\n",
      "             [ -2.45221788e-03,  -2.09528448e-03,  -8.76139331e-01,\n",
      "                8.72442462e-01,   8.29186503e-01,  -1.56147068e+00,\n",
      "               -7.01352790e-01,  -8.34482504e-01,  -8.38528207e-01,\n",
      "                4.89133563e-05,   7.05578597e-01,   4.74033749e-06]])\n",
      "Intercept: [6.13638186333,0.0160259476016,-6.15240781093]\n",
      "coefficientMatrix check = 1 \n",
      "interceptVector check = 1 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, OneVsRest, LogisticRegressionModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import tempfile\n",
    "\n",
    "# ## save and load model\n",
    "# temp_path = tempfile.mkdtemp()\n",
    "# #globs['temp_path'] = temp_path\n",
    "# reg_path = temp_path + '/reg'\n",
    "\n",
    "reg = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=1000, regParam=0.01, family=\"multinomial\" )E\n",
    "regModel = reg.fit(trainData)\n",
    "\n",
    "predict = regModel.transform(testData)\n",
    "predict.select(\"prediction\", \"label\", \"features\").show()\n",
    "#predict = model2.transform(testData)\n",
    "# predict = model2.transform(predictData)\n",
    "# predict.select(\"prediction\", \"label\", \"features\").show()\n",
    "\n",
    "# load saved model\n",
    "# reg2 = LogisticRegression.load(reg_path)\n",
    "# regModel2 = LogisticRegressionModel.load(model_path)\n",
    "# predict = regModel2.transform(predictData)\n",
    "# predict.select(\"prediction\", \"label\", \"features\").show()\n",
    "        \n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predict)\n",
    "\n",
    "## save and load model\n",
    "temp_path = tempfile.mkdtemp()\n",
    "#globs['temp_path'] = temp_path\n",
    "reg_path = temp_path + '/reg'\n",
    "reg.save(reg_path)\n",
    "model2 = LogisticRegression.load(reg_path)\n",
    "model2.getMaxIter()\n",
    "\n",
    "model_path = temp_path + 'reg_model'\n",
    "regModel.save(model_path)\n",
    "model2 = LogisticRegressionModel.load(model_path)\n",
    "\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "print(\"Accuracy = %g Percent\" % (accuracy * 100))\n",
    "print(\"Coefficients: \\n\" + str(regModel.coefficientMatrix))\n",
    "print(\"Intercept: \" + str(regModel.interceptVector))\n",
    "print(\"coefficientMatrix check = %g \" % (regModel.coefficientMatrix[0, 1] == model2.coefficientMatrix[0, 1]))\n",
    "print(\"interceptVector check = %g \" % (regModel.interceptVector == model2.interceptVector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labelCol: label column name. (default: label, current: label)\n",
      "metricName: metric name in evaluation (f1|weightedPrecision|weightedRecall|accuracy) (default: f1, current: accuracy)\n",
      "predictionCol: prediction column name. (default: prediction, current: prediction)\n"
     ]
    }
   ],
   "source": [
    "print evaluator.explainParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(label=0.0, features=SparseVector(12, {0: 1.0, 1: 1.0, 2: 1.0, 4: 1.0, 9: 236.0, 11: 386.0}), Gender=u'Male', Account_Type=u'Current Account', Age=u'60 +', Education=u'Tertiary and above', Employment=u'Permanent', Salary=236, Employer_Stability=u'Stable', Customer_Loyalty=0, Balance=386, Residential_Status=u'Owned', Service_Level=0.0, rawPrediction=DenseVector([6.1675, 0.0231, -6.1905]), probability=DenseVector([0.9979, 0.0021, 0.0]), prediction=0.0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regModel.transform(testData).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict = model2.transform(predictData)\n",
    "# predict.select(\"prediction\", \"label\", \"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os.path import join as pjoin\n",
    "\n",
    "## save and load model\n",
    "#temp_path = tempfile.mkdtemp()\n",
    "temp_path = pjoin(\"/home/maffsojah/Projects/HIT_400/capstone_project/web/tbank/spark-warehouse\")\n",
    "#globs['temp_path'] = temp_path\n",
    "reg_path = temp_path + '/reg'\n",
    "reg.save(reg_path)\n",
    "#model2 = LogisticRegression.load(reg_path)\n",
    "#model2.getMaxIter()\n",
    "\n",
    "model_path = temp_path + 'reg_model'\n",
    "regModel.save(model_path)\n",
    "#model2 = LogisticRegressionModel.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = LogisticRegression.load(reg_path)\n",
    "model2.getMaxIter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
